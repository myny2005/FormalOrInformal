{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a6bbec9-76e4-4a9a-ba2b-9f6c21887ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9913c383-5a57-4605-afca-5662be57c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\" # \"small\" bert\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 8 \n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device(\"cpu\")  # or cuda (I don't have cuda :( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a53bc699-fbd2-4732-ad9c-6e7082b4adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1099, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/labeled_texts.csv\")\n",
    "df[\"label\"] = df[\"label\"].map({\"formal\": 0, \"informal\": 1})\n",
    "train_df, _ = train_test_split(df, test_size = 0.15, stratify = df[\"label\"], random_state = 123321)\n",
    "print(train_df.shape)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7df2dfc-6386-47bf-9009-eeab4306b975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 6892, 10024, 7076, 2003, 1037, 4658, 2194, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer testing\n",
    "sample_text = \"Jetbrains is a cool company!\"\n",
    "tokens = tokenizer(sample_text, padding = \"max_length\", truncation = True, max_length = 128)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78fd42fe-13ac-4237-af5e-a8cc50c717ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts, truncation = True, padding = \"max_length\", max_length = MAX_LENGTH, return_tensors = \"pt\")\n",
    "\n",
    "class FormalityDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.encodings = tokenize_texts(dataframe[\"text\"].tolist())\n",
    "        self.labels = torch.tensor(dataframe[\"label\"].tolist())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc6e400e-446f-4194-8208-0c8a51533a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FormalityDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels = 2)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 3e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a6d9fb7-8575-4335-a560-1eb82399e210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 138/138 [03:30<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg loss: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████| 138/138 [03:35<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg loss: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████| 138/138 [03:36<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg loss: 0.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/distilbert/tokenizer_config.json',\n",
       " '../models/distilbert/special_tokens_map.json',\n",
       " '../models/distilbert/vocab.txt',\n",
       " '../models/distilbert/added_tokens.json',\n",
       " '../models/distilbert/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc = f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Avg loss: {total_loss/len(train_loader):.2f}\")\n",
    "\n",
    "model.save_pretrained(\"../models/distilbert\")\n",
    "tokenizer.save_pretrained(\"../models/distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42492069-fc3d-45c3-b94d-fb19d2eac0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
